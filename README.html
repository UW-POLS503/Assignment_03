<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Assignment 03</title>

<script src="README_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="README_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="README_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="README_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="README_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="README_files/htmlwidgets-0.6/htmlwidgets.js"></script>
<script src="README_files/datatables-binding-0.1/datatables.js"></script>
<script src="README_files/datatables-1.10.7/jquery.dataTables.min.js"></script>
<link href="README_files/datatables-default-1.10.7/dataTables.extra.css" rel="stylesheet" />
<link href="README_files/datatables-default-1.10.7/jquery.dataTables.min.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="README_files/highlight/default.css"
      type="text/css" />
<script src="README_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="README_files/navigation-1.0/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">



<h1 class="title">Assignment 03</h1>

</div>


<p><span class="math display">\[
\DeclareMathOperator{\cor}{cor}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\quantile}{quantile}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\renewcommand{\mat}[1]{\boldsymbol{#1}}
\]</span></p>
<div id="instructions" class="section level2">
<h2>Instructions</h2>
<ol style="list-style-type: decimal">
<li><a href="https://help.github.com/articles/using-pull-requests/">Fork this repository</a> to your GitHub account.</li>
<li>Write your solutions in R Markdown in a file named <code>solutions.Rmd</code>.</li>
<li>When you are ready to submit your assignment, <a href="https://help.github.com/articles/using-pull-requests/#initiating-the-pull-request">initiate a pull request</a>. Title your pull request “Submission”.</li>
</ol>
<p>To update your fork from the upstream repository:</p>
<ol style="list-style-type: decimal">
<li>On your fork, e.g. <code>https://github.com/jrnold/Assignment_03</code> click on “New Pull reqest”</li>
<li>Set your fork <code>jrnold/Assignment_03</code> as the base fork on the left, and <code>UW-POLS503/Assignment_03</code> as the head fork on the right. In both cases the branch will be master. This means, compare any chanes in the head fork that are not in the base fork. You will see differences between the <code>US-POLS503</code> repo and your fork. Click on “Create Pull Request”, and if there are no issues, “Click Merge” A quick way is to use this link, but change the <code>jrnold</code> to your own username: <code>https://github.com/jrnold/Assignment_03/compare/master...UW-POLS503:master</code>.</li>
</ol>
<p>We’ll use these packages,</p>
<pre class="r"><code>library(&quot;foreign&quot;)
library(&quot;dplyr&quot;)
library(&quot;broom&quot;)
library(&quot;ggplot2&quot;)
library(&quot;DT&quot;)</code></pre>
<p>Since we are going to do some simulation, we shoudl set a seed, so the results are exactly replicable.</p>
<pre class="r"><code>set.seed(1234)</code></pre>
<p>Since some of these computations will take time, we can cache the results so that knitr will only run code that has changed.</p>
<pre class="r"><code>knitr::opts_chunk$set(cache = TRUE, autodep = TRUE)</code></pre>
</div>
<div id="nunn-and-wantchekon-aer-2011-example" class="section level2">
<h2>Nunn and Wantchekon AER 2011 example</h2>
<p>Let’s run some regressions from</p>
<blockquote>
<p>Nunn, Nathan and Leonard Wantchekon. 2011. “The Slave Trade and the Origins of Mistrust in Africa.” American Economic Review, 101(7):3221-52. <a href="https://dx.doi.org/10.1257/aer.101.7.3221"><a href="doi:10.1257/aer.101.7.3221" class="uri">doi:10.1257/aer.101.7.3221</a></a></p>
</blockquote>
<p>The replication data for the is available from its <a href="https://dx.doi.org/10.1257/aer.101.7.3221">AER site</a>, but the main dataset is included in this repository. Since the main dataset is a Stata <code>.dta</code> file load it using the <code>read.dta</code> function and convert it to a <strong>dplyr</strong> <code>tbl</code> so the <code>print</code> function produces nicer output.</p>
<pre class="r"><code>nunn &lt;- read.dta(&quot;Nunn_Wantchekon_AER_2011.dta&quot;) %&gt;% tbl_df()</code></pre>
<p>There are many variables in this data. When <code>read.dta</code> converts a Stata data file the descriptions of the variables end up in an R <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/attributes.html">attribute</a> <code>&quot;var.labels&quot;</code>. Print out the variable lables to get the descriptions of the files<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<pre class="r"><code>data_frame(variable = names(nunn), description = attr(nunn, &quot;var.labels&quot;)) %&gt;%
  datatable(class = &#39;cell-border stripe&#39;)</code></pre>
<p>preserve7112d7ba957dd2f8</p>
<p>In Table 1, NW run several models with Trust in Neighbors as an outcome variable, different measures of slave exports as the treatment variable, and the same set of controls variables. The relevant variables in the data are:</p>
<ul>
<li><code>trust_neighbors</code>: Trust of neighbors</li>
<li><code>exports</code>: Slave exports in 1000s</li>
<li>Individual controls: <code>age</code>, <code>age2</code>, <code>male</code>, <code>urban_dum</code>, <code>education</code>, <code>occupation</code>, <code>religion</code>, <code>living_conditions</code></li>
<li>District controls: <code>district_ethnic_frac</code>, <code>frac_ethnicity_in_district</code></li>
<li>Country-fixed effects: <code>isocode</code></li>
</ul>
<div id="bivariate-regression" class="section level3">
<h3>Bivariate regression</h3>
<p>Run a regression of the Trust of Neighbors on Slave exports. This is Table 1, Model 1, witout any of the control variables.</p>
<pre class="r"><code>mod1 &lt;- lm(trust_neighbors ~ exports, data = nunn)</code></pre>
<div class="bs-callout bs-callout-info">
<ul>
<li>Interpret the magnitude and statistical significance of the coefficient on <code>trust_neighbors</code>.</li>
<li>Plot the fitted values and confidence interval of the fitted values of regression vs. <code>exports</code>.</li>
<li>Plot the residuals of this regresion against the fitted values of the regression. Do they appear to have constant variance? Are they approximately symmetric?</li>
<li>What is the null hypothesis of the t-test reported by <code>summary()</code>? Explain the meaning of the p-value. Be precise. Is the p-value the probability that the null hypothesis is correct?</li>
</ul>
</div>
<p>Frequentist statistics assigns no probabilities to hypotheses (parameter values). They are either true or false, but they are unknown. Only samples are random variables, and have an associated probability. But as scientists, we are generally interested in the probability that a hypothesis is correct.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> The probability that the research hypothesis (<span class="math inline">\(H_0\)</span>) is correct can be calculated with Bayes law, <span class="math display">\[
p(H_0 | \text{data}) = \frac{p(\text{data | H_0} p(H_0))}{p(\text{data} | H_a) p(H_a) + p(\text{data} | H_0) p(H_0)} = \frac{p(\text{data | H_a} p(H_a))}{p(\text{data})} .
\]</span> Working somewhat informally, the p-value gives <span class="math inline">\(p(\text{data} | H_0)\)</span>. An important missing piece of information is the baseline or prior probabilty that the null hypothesis is true, <span class="math inline">\(p(H_0)\)</span>, which is the complement of the probability that the research hypothesis is true, <span class="math inline">\(p(H_0) = 1 - p(H_a)\)</span>,<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> <a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<div class="bs-callout bs-callout-info">
<ul>
<li>If more than the p-value is required to make sense of the research findings, what does the article do to increase your belief about <span class="math inline">\(p(H_a)\)</span>?</li>
<li>Suppose you believed that NW were p-value hacking (which I don’t think they are!). What part of Bayes law is that affecting? If you think that someone is p-value hacking, then you are saying that they will always produce significant p-values regardless of whether the null or alternative hypotheses are true.</li>
</ul>
</div>
<p>In the models in Table 1, NW includes control variables to account for individual, district, and country-level variables that may explain differences.</p>
</div>
<div id="multiple-regression" class="section level3">
<h3>Multiple regression</h3>
<div class="bs-callout bs-callout-info">
<ul>
<li>What are the control variables that NW include in the models in Table 1?</li>
<li>Run the model in Table 1, Regression 1 and report the results: coefficients, p-values, etc.</li>
<li>Interpret the coefficient on <code>exports</code></li>
<li>How much does the coefficient change with the addition of control variables? What does that suggest?</li>
<li>Do the R^2 and number of observations match those reported in Table 1?</li>
<li>Calculate the fitted values of the regression by multiplying the <span class="math inline">\(\beta\)</span> vector and the <span class="math inline">\(\mat{X}\)</span> matrix. Confirm that you get the same results as using <code>predict()</code>.</li>
<li>How would you create a plot that shows the predicted values of <code>trust_neighbors</code> as the value of <code>exports</code> changes? What is different about the multiple regression case than the bivariate case?</li>
</ul>
</div>
</div>
<div id="understanding-multiple-regression" class="section level3">
<h3>Understanding Multiple Regression</h3>
<div class="bs-callout bs-callout-info">
<ul>
<li>Run the following regresssions
<ol style="list-style-type: decimal">
<li>Run the regression of <code>trust_neighbors</code> on the controls. Save the residuals</li>
<li>Run the regression of <code>exports</code> on the controls. Save the residuals</li>
<li>Regression the residuals from regression 1 on the residuals from regression 2.</li>
</ol></li>
<li>How does the coefficient in regression 3 compare the the coefficient on <code>exports</code> from the regression in Table 1, Model 6? What does that say about what multiple regression is doing?</li>
</ul>
</div>
</div>
<div id="validity-of-the-standard-errors" class="section level3">
<h3>Validity of the standard errors</h3>
<p>One of the assumptions necessary for OLS standard errors to be correct is homoskedasticity homoskedasticity (constant variance), and that the errors are uncorrelated.</p>
<div class="bs-collout"
- How might that assumption be violated?
- Plot the residuals of the regression by district. Do they appear to be uncorrelated? What does that say about the validity of the OLS standard errors?
- Do the standard errors match those in Table 1? What sort of standard errors does the article use?
</div>

</div>
<div id="log-slave-exports-per-capita" class="section level3">
<h3>log slave exports per capita</h3>
<div>
<ul>
<li>Run the regression in Table 1, model 6, which uses “log(1 + exports / pop)” as a measure of slave exports.</li>
<li>Interpret the effect of this variable</li>
<li>Why is “log(1 + exports / pop)” used as the measure instead of “log(exports / pop)”?</li>
<li>Plot the fitted values of log(1 + exports / pop) and their confidence interval against “log(1 + exports / pop)” against the residuals of the controls only regression. Include the line, confidence intervals, and data points.</li>
<li>Plot the fitted values of exports / pop against the residuals of the controls only regression. Include the line, confidence intervals, and data points. How does this relationship differ from the one which used the level of slave exports with out taking the logarithm or adjusting for population?</li>
</ul>
</div>
</div>
<div id="sampling-distribution-of-coefficients" class="section level3">
<h3>Sampling distribution of coefficients</h3>
<p>Let’s understand what the confidence intervals mean in terms of the sampling distribution. Since we don’t know the true parameter values for this, we will pretend that the OLS point estimates are the “true” population parameters.</p>
<p>Given an OLS model with estimated <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\sigma}\)</span>, and data, the function does <code>resampler_coef</code>,</p>
<ol style="list-style-type: decimal">
<li>Sample <span class="math inline">\(y^* \sim N(\hat{\beta} X, \hat{\sigma}^2)\)</span></li>
<li>Estimate OLS estimates $ = ^* X + <span class="math inline">\(\varepsilon^*\)</span></li>
<li>Repeat steps 1–2, <code>iter</code> times, store <span class="math inline">\(\beta^*\)</span> for each iteration, and return the estimates for all samples.</li>
</ol>
<p>The distribution of the <span class="math inline">\(\beta^*\)</span> is a sampling distribution of the parameters.</p>
<pre class="r"><code>resampler_coef &lt;- function(mod, .data, iter = 1) {
  # Remove missing values
  .data &lt;- na.omit(.data)
  # Coefficients
  beta &lt;- coef(mod)
  # mod$terms contains the formula used in the regression
  X &lt;- model.matrix(mod$terms, data = .data)
  # estimate of std. dev. of errors
  sigma &lt;- sqrt(sum(mod$residuals ^ 2) / mod$df.residual)
  # This produces the same result
  # sigma &lt;- summary(mod1)$sigma  
  # Number of observations
  n &lt;- nrow(X)
  # Name of dependent variable
  outcome_var_name &lt;- all.vars(mod$terms)[1]
  # List to save results
  results &lt;- vector(mode = &quot;list&quot;, length = iter)
  for (i in seq_len(iter)) {
    # draw errors
    errors &lt;- rnorm(n, mean = 0, sd = sigma)
    # create new outcome variable from errors
    y &lt;- X %*% beta + errors
    # replace outcome variable
    .data[[outcome_var_name]] &lt;- y
    # run regression
    newmod &lt;- lm(mod$terms, data = .data)
    # Save coefficients as a data frame to the list
    results[[i]] &lt;- tidy(newmod) %&gt;% mutate(.iter = i)
  }
  # Convert the list of data frames to a single data frame by stacking the iterations
  bind_rows(results)
}</code></pre>
<p>Suppose that <code>mod_1_6</code> is the model containing the results of the model in Table 1, Model 6. Simulate the sampling distribution of the</p>
<pre class="r"><code>beta_dist &lt;- resampler_coef(mod_1_6, nunn, iter = 1024)
head(beta_dist)</code></pre>
<div class="bs-callout bs-callout-info">
<ul>
<li>Plot the distributions of the coefficients of exports.</li>
<li>Calculate the correlation matrix of the coefficients. How similar is it to that returned from `vcov(mod)</li>
</ul>
</div>
<p>TODO: clarify this</p>
</div>
<div id="bootstrapping" class="section level3">
<h3>Bootstrapping</h3>
<p>What you just did is an example of parametric bootstrap. It is a parametric bootstrap because you drew data from an assumed model (the OLS model that you estimated).</p>
<p>An alternative is a non-parametric bootstrap. In a non-parametric bootstrap, instead of drawing samples from model, we are going to redraw samples from the sample.</p>
<div class="figure">
<img src="head-exploding-gif-tim-and-eric.gif" alt="Head Exploding Gif" />
<p class="caption">Head Exploding Gif</p>
</div>
<p>An analogy is that the sample is to the population as the bootstrap is to the sample. We are treating the sample distribution as an estimate of the populution distribution and then drawing samples from that estimated population distribution.</p>
<p>To do the bootstrapping we will use the <code>bootstrap</code> function in the <strong>tidyr</strong> package. However, the <a href="https://cran.r-project.org/web/packages/boot/index.html">boot</a> package supports many more advanced methods of bootstrapping.</p>
<p>Let’s start by drawing a single bootstrap replication. It is a sample of the same size as the original data, drawn from the data <em>with replacement</em>.</p>
<pre class="r"><code>nunn_bootstrapped &lt;- bootstrap(nunn, 1)</code></pre>
<p>So, in order to calculate boostrapped standard erorrs, we will need to draw a sample of To get bootstrap standard errors, we draw <code>B</code> replications, run an regression, and save the estimates.</p>
<pre class="r"><code>beta_bs &lt;- 
  bootstrap(nunn, 1024) %&gt;%
    do(tidy(lm(trust_neighbors ~ exports, data = nunn)))</code></pre>
<p>There are several ways to calculate standard errors from bootstraped replications. The following are two simple methods.</p>
<ol style="list-style-type: decimal">
<li><p>Calculate the standard error from these simulations by taking the standard deviation of the estimates. Suppose <span class="math inline">\(\beta^{*b}_k\)</span> is the estimated cofficient from replication <span class="math inline">\(b \in 1:B\)</span>, and <span class="math inline">\(\bar\tilde^{*}_k = (\sum \beta^{*b}_k) / B\)</span>. Then the bootstrap standard error is, <span class="math display">\[
   \se_k(\hat\beta_{k}) = \sqrt{\frac{1}{B - 1} \sum (\beta^{*b}_k - \bar\beta^{*b}_k)^2}
   \]</span> The confidence interval is thus, <span class="math display">\[
   \hat{\beta}_k \pm \se_{bs}(\hat\beta_k)
   \]</span> Note that you use the estimate <span class="math inline">\(\hat{\beta}_k\)</span> from the original model, not the mean of the bootstrap estimates. This method works well if the sampling distribution of <span class="math inline">\(\beta_k\)</span> is symmetric.</p></li>
<li><p>The second method is to use the quantiles of the bootstap estimates. E.g. a 95% confidence interval uses the 2.5% and 97.5% quantiles of the boostrap estimates. This method allows for asymmetric confidence intervals. However, it takes more replications to get accurate values of extreme quantiles than it does to calculate a standard deviation.</p></li>
</ol>
<div class="bs-callout bs-callout-info">
<ul>
<li>Estimate the bootstrapped confidence intervals using those two methods.</li>
<li>Compare the bootstrapped confidence intervals to the OLS confidence interval.</li>
</ul>
</div>
<p>There are even more advanced methods such as the studentized bootstrap, and the adjusted boostrap percentile (BCa) methods included in <code>boot.ci</code>.</p>
<p>For bootstrapped standard errors to be valid, the samples from the data need to be taken in the same way as the sample was taken from the population. For example, in a time series it would be inappropriate to sample observations without accounting for their order.</p>
<div class="bs-callout bs-callout-info">
<ul>
<li>What is the population in this paper?</li>
<li>How was the sample drawn from this population?</li>
<li>In the previous examples, did we draw the sample in the same way as it was drawn from the population? What would be a better way of drawing the bootstrapped samples? Try to implement it, and compare the differences.</li>
</ul>
</div>
</div>
<div id="f-test-example" class="section level3">
<h3>F-test example</h3>
<p>TODO: Fix this</p>
<p>An <span class="math inline">\(F\)</span>-test tests the null hypthosis that several coefficients in the regression are all 0 vs. the alternative that at least one of the coefficients is non-zero.</p>
<p><span class="math display">\[
\begin{aligned}[t]
H_0: &amp;\quad \beta_j = \dots = \beta_J = 0
H_a: &amp;\quad \text{at least one $\beta_k \neq 0$}
\end{aligned}
\]</span></p>
<ul>
<li>Run F-tests of the multiple regression model vs. the model with no controls.</li>
<li>Run and interpet an F-test on some reasonable group of variables.</li>
</ul>
<p>F-test simulations</p>
<pre class="r"><code>resampler_models &lt;- function(mod, .data, iter = 1) {
  # Remove missing values
  .data &lt;- na.omit(.data)
  # Coefficients
  beta &lt;- coef(mod)
  # mod$terms contains the formula used in the regression
  X &lt;- model.matrix(mod$terms, data = .data)
  # estimate of std. dev. of errors
  sigma &lt;- sqrt(sum(mod$residuals ^ 2) / mod$df.residual)
  # This produces the same result
  # sigma &lt;- summary(mod1)$sigma  
  # Number of observations
  n &lt;- nrow(X)
  # Name of dependent variable
  outcome_var_name &lt;- all.vars(mod$terms)[1]
  # List to save results
  results &lt;- vector(mode = &quot;list&quot;, length = iter)
  for (i in seq_len(iter)) {
    # draw errors
    errors &lt;- rnorm(n, mean = 0, sd = sigma)
    # create new outcome variable from errors
    y &lt;- X %*% beta + errors
    # replace outcome variable
    .data[[outcome_var_name]] &lt;- y
    # run regression
    newmod &lt;- lm(mod$terms, data = .data)
    # Save model stats as a data frame to the list
    results[[i]] &lt;- glimpse(newmod) %&gt;% mutate(.iter = i)
  }
  # Convert the list of data frames to a single data frame by stacking the iterations
  bind_rows(results)
}</code></pre>
</div>
</div>
<div id="multiple-comparisons-and-the-f-test" class="section level2">
<h2>Multiple comparisons and the F-test</h2>
<pre class="r"><code>noise &lt;- data.frame(matrix(rnorm(2100), nrow = 100, ncol = 21))
summary(lm(noise))</code></pre>
<div class="bs-callout bs-callout-info">
<ul>
<li>How many variables have t-tests that are significant? Is this to be expected?</li>
<li>Is the F-test significant?</li>
<li>What is the null and alternative hypotheses of the F-test? What are the implicit null and alternative hypotheses of running all those t-tests? What explains the difference in results?</li>
</ul>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This uses the <a href="https://rstudio.github.io/DT/">DT</a> package to produce pretty interactive tables in the HTML.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>See the discussion in (Scientific method: Statistical errors)[<a href="http://www.nature.com/news/scientific-method-statistical-errors-1.14700" class="uri">http://www.nature.com/news/scientific-method-statistical-errors-1.14700</a>], <em>Nature</em>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Assuming, for simplicity, that <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_a\)</span> are the only hypotheses so that <span class="math inline">\(p(H_0) + p(H_a) = 1\)</span>.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>This question is non-standard and idiosyncratic to the way I interpret research.<a href="#fnref4">↩</a></p></li>
</ol>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
