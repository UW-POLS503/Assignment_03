<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Assignment 03</title>

<script src="README_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="README_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="README_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="README_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="README_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="README_files/htmlwidgets-0.6/htmlwidgets.js"></script>
<script src="README_files/datatables-binding-0.1/datatables.js"></script>
<script src="README_files/datatables-1.10.7/jquery.dataTables.min.js"></script>
<link href="README_files/datatables-default-1.10.7/dataTables.extra.css" rel="stylesheet" />
<link href="README_files/datatables-default-1.10.7/jquery.dataTables.min.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="README_files/highlight/default.css"
      type="text/css" />
<script src="README_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="README_files/navigation-1.0/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->





$$
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

\newcommand{\distr}[1]{\mathcal{#1}}
\newcommand{\dnorm}{\distr{N}}
\newcommand{\dmvnorm}[1]{\distr{N}_{#1}}
$$

<div class="fluid-row" id="header">



<h1 class="title">Assignment 03</h1>

</div>


<p>Instructions</p>
<ol style="list-style-type: decimal">
<li><a href="https://help.github.com/articles/using-pull-requests/">Fork this repository</a> to your GitHub account.</li>
<li>Write your solutions in R Markdown in a file named <code>solutions.Rmd</code>.</li>
<li>When you are ready to submit your assignment, <a href="https://help.github.com/articles/using-pull-requests/#initiating-the-pull-request">initiate a pull request</a>. Title your pull request “Submission”.</li>
</ol>
<p>To update your fork from the upstream repository:</p>
<ol style="list-style-type: decimal">
<li>On your fork, e.g. <code>https://github.com/jrnold/Assignment_03</code> click on “New Pull request”</li>
<li>Set your fork <code>jrnold/Assignment_03</code> as the base fork on the left, and <code>UW-POLS503/Assignment_03</code> as the head fork on the right. In both cases the branch will be master. This means, compare any canes in the head fork that are not in the base fork. You will see differences between the <code>US-POLS503</code> repository and your fork. Click on “Create Pull Request”, and if there are no issues, “Click Merge” A quick way is to use this link, but change the <code>jrnold</code> to your own username: <code>https://github.com/jrnold/Assignment_03/compare/master...UW-POLS503:master</code>.</li>
</ol>
<p>We’ll use these packages,</p>
<pre class="r"><code>library(&quot;foreign&quot;)
library(&quot;dplyr&quot;)
library(&quot;broom&quot;)
library(&quot;ggplot2&quot;)
library(&quot;DT&quot;)</code></pre>
<p>Since we are going to do some simulation, we should set a seed, so the results are exactly replicable.</p>
<pre class="r"><code>set.seed(1234)</code></pre>
<p>Since some of these computations will take time, we can cache the results so that knitr will only run code that has changed.</p>
<pre class="r"><code>knitr::opts_chunk$set(cache = TRUE, autodep = TRUE)</code></pre>
<div id="nunn-and-wantchekon-aer-2011-example" class="section level1">
<h1><span class="header-section-number">1</span> Nunn and Wantchekon AER 2011 example</h1>
<p>Let’s run some regressions from</p>
<blockquote>
<p>Nunn, Nathan and Leonard Wantchekon. 2011. “The Slave Trade and the Origins of Mistrust in Africa.” American Economic Review, 101(7):3221-52. <a href="https://dx.doi.org/10.1257/aer.101.7.3221"><a href="doi:10.1257/aer.101.7.3221" class="uri">doi:10.1257/aer.101.7.3221</a></a></p>
</blockquote>
<p>The replication data for the is available from its <a href="https://dx.doi.org/10.1257/aer.101.7.3221">AER site</a>, but the main dataset is included in this repository. Since the main dataset is a Stata <code>.dta</code> file load it using the <code>read.dta</code> function and convert it to a <strong>dplyr</strong> <code>tbl</code> so the <code>print</code> function produces nicer output.</p>
<pre class="r"><code>nunn &lt;- read.dta(&quot;Nunn_Wantchekon_AER_2011.dta&quot;) %&gt;% tbl_df()</code></pre>
<p>There are many variables in this data. When <code>read.dta</code> converts a Stata data file the descriptions of the variables end up in an R <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/attributes.html">attribute</a> <code>&quot;var.labels&quot;</code>. Print out the variable labels to get the descriptions of the files<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<pre class="r"><code>data_frame(variable = names(nunn), description = attr(nunn, &quot;var.labels&quot;)) %&gt;%
  datatable(class = &#39;cell-border stripe&#39;)</code></pre>
<p><div id="htmlwidget-1137" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1137">{"x":{"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59"],["respno","ethnicity","murdock_name","isocode","region","district","townvill","location_id","trust_relatives","trust_neighbors","intra_group_trust","inter_group_trust","trust_local_council","ln_export_area","export_area","export_pop","ln_export_pop","age","age2","male","urban_dum","occupation","religion","living_conditions","education","near_dist","distsea","loc_murdock_name","loc_ln_export_area","local_council_performance","council_listen","corrupt_local_council","school_present","electricity_present","piped_water_present","sewage_present","health_clinic_present","district_ethnic_frac","frac_ethnicity_in_district","townvill_nonethnic_mean_exports","district_nonethnic_mean_exports","region_nonethnic_mean_exports","country_nonethnic_mean_exports","murdock_centr_dist_coast","centroid_lat","centroid_long","explorer_contact","railway_contact","dist_Saharan_node","dist_Saharan_line","malaria_ecology","v30","v33","fishing","exports","ln_exports","total_missions_area","ln_init_pop_density","cities_1400_dum"],["Respondent number in Afrobarometer dataset","Ethnicity name: from Afrobarometer q79","Ethnicity name: from Murdock","Country 3 digit iso code","Region: from Afrobarometer","District: from Afrobarometer","Town/village: from Afrobarometer","Unique location-of-repondent identifier - based on isocode region district townv","Trust of relatives: q84a","Trust of neighbors: q84b","Intra-group trust: q84d","Inter-group trust: q84c","Trust of local government council: q55d","Log [(total slave exports: Atlantic + Indian) / area (km^2)","(total slave exports: Atlantic + Indian) / area (km^2)","Exports divided by historic Murdock population","Ln (1+exports/Murdock historic population)","Age: q1","Age squared","Indicator for respondent being male: q101","Indicator for respondent living in urban area","Occupation categories: q95","Religion categories: q91","Living condition categories:q4b","Education categories: Afrobarometer q90","Current distance from coast 1000s kms","Historic distance from coast 1000s kms","Murdock identifier for the current location of the respondent","Slave exports measure based on current location of respondent","Perceived performance of local council: q68c","Does the local council listen: q62b","How much corruption in local council: q56c","Is there a school in the PSU: q116b","Is there electricity in the PSU: q116d","Is there piped water in the PSU: q116e","Is there sewage in the PSU: q116f","Is there a health clinic in the PSU: q116g","District-level ethnic fractionalization","Proportion of ethnic group in district","Avg slave exports of other ethnicities within town/village","Avg slave exports of other ethnicities within district","Avg slave exports of other ethnicities within region","Avg slave exports of other ethnicities within country","Historic distance of ethnicity's centroid from the coast (in kms)","Historic latitude of centroid of ethnic group","Historic longitude of centroid of ethnic group","Indicator for historic contact with European explorer","Indicator variable for historic integration into the colonial railway network","Historic distance of ethnicity's centroid from a centroid (town) in Saharan trad","Historic distance of ethnicity's centroid from a line (route) in Saharan trade (","Ethnic groups average malaria ecology measure","Pre-colonial settlement patterns of ethnicity: from Ethngraphic Atlas v30","Pre-colonial juris. hierarchy beyond the local community: Ethnographic Atlas v33","Pre-colonial reliance on fishing: Ethnographic Atlas v3","(Atlantic+Indian Exports)","ln(1+Atlantic+Indian Exports)","Total Catholic + Protestant mission per land area","Log population density during the colonial period - from Murdock","Indicator for existence of city among ethnic group in 1400"]],"container":"<table class=\"cell-border stripe\">\n  <thead>\n    <tr>\n      <th> \u003c/th>\n      <th>variable\u003c/th>\n      <th>description\u003c/th>\n    \u003c/tr>\n  \u003c/thead>\n\u003c/table>","options":{"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}]},"callback":null,"filter":"none"},"evals":[],"jsHooks":[]}</script></p>
<p>In Table 1, NW run several models with Trust in Neighbors as an outcome variable, different measures of slave exports as the treatment variable, and the same set of controls variables. Some of the relevant variables in the data are:</p>
<ul>
<li><code>trust_neighbors</code>: Trust of neighbors</li>
<li><code>exports</code>: Slave exports in 1000s</li>
<li>Individual controls: <code>age</code>, <code>age2</code>, <code>male</code>, <code>urban_dum</code>, <code>education</code>, <code>occupation</code>, <code>religion</code>, <code>living_conditions</code></li>
<li>District controls: <code>district_ethnic_frac</code>, <code>frac_ethnicity_in_district</code></li>
<li>Country-fixed effects: <code>isocode</code></li>
</ul>
<p>Note that NW use <code>education</code>, <code>occupation</code>, <code>religion</code>, and <code>living_conditions</code> as factor variables. Convert them accordingly,</p>
<pre class="r"><code>factor_vars &lt;- c(&quot;education&quot;, &quot;occupation&quot;, &quot;religion&quot;, &quot;living_conditions&quot;)
for (i in factor_vars) {
  nunn[[i]] &lt;- factor(nunn[[i]])
}</code></pre>
<div id="bivariate-regression" class="section level2">
<h2><span class="header-section-number">1.1</span> Bivariate regression</h2>
<p>Run a regression of the Trust of Neighbors on Slave exports. This is Table 1, Model 1, without any of the control variables.</p>
<pre class="r"><code>mod_1_0 &lt;- lm(trust_neighbors ~ exports, data = nunn)</code></pre>
<div class="bs-callout bs-callout-info">
<ul>
<li>Interpret the magnitude and statistical significance of the coefficient on <code>trust_neighbors</code>.</li>
<li>Plot the fitted values and confidence interval of the fitted values of regression vs. <code>exports</code>.</li>
<li>Plot the residuals of this regression against the fitted values of the regression. Do they appear to have constant variance? Are they approximately symmetric?</li>
<li>What is the null hypothesis of the t-test reported by <code>summary()</code>? Explain the meaning of the p-value. Be precise. Is the p-value the probability that the null hypothesis is correct?</li>
</ul>
</div>
<p>Example of using <code>augment</code> for fitted values:</p>
<pre class="r"><code>augment(mod_1_0, data_frame(exports = seq(min(nunn$exports, na.rm = TRUE), max(nunn$exports, na.rm = TRUE), length.out = 100)))</code></pre>
<p>Example of plotting the confidence intervals</p>
<pre class="r"><code>ggplot() + geom_line(data = mod_1_0_fitted, mapping = aes(x = exports, y = .fitted)) + geom_ribbon(data = mod1_fitted, mapping = aes(x = exports, y = .fitted, ymin = .fitted - 2 * .se.fit, ymax = .fitted + 2 * .se.fit), alpha = 0.3) + geom_point(data = nunn, aes(x = exports, y = trust_neighbors))</code></pre>
</div>
<div id="probablities-of-hypotheses" class="section level2">
<h2><span class="header-section-number">1.2</span> Probablities of Hypotheses</h2>
<p>Frequentist statistics assigns no probabilities to hypotheses (parameter values). They are either true or false, but they are unknown. Only samples are random variables, and have an associated probability. But as scientists, we are generally interested in the probability that a hypothesis is correct.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> The probability that the research hypothesis (<span class="math inline">\(H_0\)</span>) is correct can be calculated with Bayes law, <span class="math display">\[
p(H_0 | \text{data}) =
\frac{p(\text{data} | H_0) p(H_0)}{p(\text{data} | H_a) p(H_a) + p(\text{data} | H_0) p(H_0)} = \frac{p(\text{data} | H_0) p(H_0)}{p(\text{data})}
\]</span> Working somewhat informally, the p-value gives <span class="math inline">\(p(\text{data} | H_0)\)</span>. An important missing piece of information is the baseline or prior probability that the null hypothesis is true, <span class="math inline">\(p(H_0)\)</span>, which is the complement of the probability that the research hypothesis is true, <span class="math inline">\(p(H_0) = 1 - p(H_a)\)</span>,<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> <a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<div class="bs-callout bs-callout-info">
<ul>
<li>If more than the p-value is required to make sense of the research findings, what does the article do to increase your belief about the research hypothesis, <span class="math inline">\(p(H_a)\)</span>?</li>
<li>Suppose you believed that NW were p-value hacking (which I don’t think they are!). What part of Bayes law is that affecting? If you think that someone is p-value hacking, then you are saying that they will always produce significant p-values regardless of whether the null or alternative hypotheses are true.</li>
</ul>
</div>
</div>
<div id="multiple-regression" class="section level2">
<h2><span class="header-section-number">1.3</span> Multiple regression</h2>
<p>In the models in Table 1, NW includes control variables to account for individual, district, and country-level variables that may explain differences.</p>
<p>Run the model in Table 1, Model 1:</p>
<pre class="r"><code>mod_1_1 &lt;- lm(trust_neighbors ~
              exports +
              # controls
              # individual level
              age + age2 + male + urban_dum + education +
              occupation + religion + living_conditions +
              # district-level 
              district_ethnic_frac + frac_ethnicity_in_district +
              # country-level
              isocode,
              data = nunn)
mod_1_1</code></pre>
<pre><code>## 
## Call:
## lm(formula = trust_neighbors ~ exports + age + age2 + male + 
##     urban_dum + education + occupation + religion + living_conditions + 
##     district_ethnic_frac + frac_ethnicity_in_district + isocode, 
##     data = nunn)
## 
## Coefficients:
##                (Intercept)                     exports  
##                  1.620e+00                  -6.791e-04  
##                        age                        age2  
##                  8.396e-03                  -5.473e-05  
##                       male                   urban_dum  
##                  4.550e-02                  -1.405e-01  
##                 education1                  education2  
##                  1.710e-02                  -5.225e-02  
##                 education3                  education4  
##                 -1.374e-01                  -1.890e-01  
##                 education5                  education6  
##                 -1.893e-01                  -2.401e-01  
##                 education7                  education8  
##                 -2.851e-01                  -1.232e-01  
##                 education9                 occupation1  
##                 -2.406e-01                   6.186e-02  
##                occupation2                 occupation3  
##                  7.392e-02                   3.356e-02  
##                occupation4                 occupation5  
##                  7.942e-03                   6.661e-02  
##                occupation6                 occupation7  
##                 -7.563e-02                   1.700e-02  
##                occupation8                 occupation9  
##                 -9.428e-02                  -9.981e-02  
##               occupation10                occupation11  
##                 -3.307e-02                  -2.300e-02  
##               occupation12                occupation13  
##                 -1.565e-01                  -1.441e-02  
##               occupation14                occupation15  
##                 -5.566e-02                  -2.344e-01  
##               occupation16                occupation18  
##                 -1.307e-02                  -1.730e-01  
##               occupation19                occupation20  
##                 -1.770e-01                  -2.458e-02  
##               occupation21                occupation22  
##                 -4.937e-02                  -1.069e-01  
##               occupation23                occupation24  
##                 -9.712e-02                   1.292e-02  
##               occupation25               occupation995  
##                  2.623e-02                  -1.195e-03  
##                  religion2                   religion3  
##                  5.396e-02                   7.888e-02  
##                  religion4                   religion5  
##                  4.749e-02                   4.318e-02  
##                  religion6                   religion7  
##                 -1.788e-02                  -3.617e-02  
##                 religion10                  religion11  
##                  6.015e-02                   2.238e-01  
##                 religion12                  religion13  
##                  2.627e-01                  -6.813e-02  
##                 religion14                  religion15  
##                  4.674e-02                   3.845e-01  
##                religion360                 religion361  
##                  3.657e-01                   3.416e-01  
##                religion362                 religion363  
##                  8.230e-01                   3.857e-01  
##                religion995          living_conditions2  
##                  4.161e-02                   4.396e-02  
##         living_conditions3          living_conditions4  
##                  8.627e-02                   1.197e-01  
##         living_conditions5        district_ethnic_frac  
##                  1.204e-01                  -1.554e-02  
## frac_ethnicity_in_district                  isocodeBWA  
##                  1.011e-01                  -4.259e-01  
##                 isocodeGHA                  isocodeKEN  
##                  1.135e-02                  -1.820e-01  
##                 isocodeLSO                  isocodeMDG  
##                 -5.511e-01                  -3.316e-01  
##                 isocodeMLI                  isocodeMOZ  
##                  7.528e-02                   8.224e-02  
##                 isocodeMWI                  isocodeNAM  
##                  3.062e-01                  -1.398e-01  
##                 isocodeNGA                  isocodeSEN  
##                 -2.382e-01                   3.867e-01  
##                 isocodeTZA                  isocodeUGA  
##                  2.079e-01                  -6.444e-02  
##                 isocodeZAF                  isocodeZMB  
##                 -2.179e-01                  -2.173e-01</code></pre>
<div class="bs-callout bs-callout-info">
<ul>
<li>Interpret the coefficient on <code>exports</code></li>
<li>How much does the coefficient change with the addition of control variables? What does that suggest?</li>
<li>Do the R^2 and number of observations match those reported in Table 1?</li>
<li>Calculate the fitted values of the regression by multiplying the <span class="math inline">\(\beta\)</span> vector and the <span class="math inline">\(\mat{X}\)</span> matrix. Use <code>model.matrix</code> to produce the <span class="math inline">\(X\)</span> matrix, and <code>%*%</code> to multiply matrices. Confirm that you get the same results as using <code>predict()</code>.</li>
<li>How would you create a plot that shows the predicted values of <code>trust_neighbors</code> as the value of <code>exports</code> changes? What is different about the multiple regression case than the bivariate case?</li>
</ul>
</div>
<div id="understanding-multiple-regression" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Understanding Multiple Regression</h3>
<div class="bs-callout bs-callout-info">
<ul>
<li>Run the following regressions
<ol style="list-style-type: decimal">
<li><p>Regress regression of <code>trust_neighbors</code> on the controls.</p>
<pre class="r"><code>lm(trust_neighbors ~ age + age2 + male + urban_dum +
   education + occupation + religion + living_conditions +
   district_ethnic_frac + frac_ethnicity_in_district +
   isocode, data = nunn)</code></pre>
Save the residuals.</li>
<li><p>Run the regression of <code>exports</code> on the controls. Save the residuals</p>
<pre class="r"><code>lm(exports ~ age + age2 + male + urban_dum +
   education + occupation + religion + living_conditions +
   district_ethnic_frac + frac_ethnicity_in_district +
   isocode, data = nunn)</code></pre>
Save the residuals.</li>
<li>Regress the residuals from 1. on the residuals on 2.</li>
</ol></li>
<li>How does the coefficient from regression 3 compare the the coefficient on <code>exports</code> from the regression in Table 1, Model 1? What does that say about what multiple regression is doing?</li>
</ul>
</div>
</div>
</div>
<div id="validity-of-the-standard-errors" class="section level2">
<h2><span class="header-section-number">1.4</span> Validity of the standard errors</h2>
<p>One of the assumptions necessary for OLS standard errors to be correct is homoskedasticity homoskedasticity (constant variance), and that the errors are uncorrelated.</p>
<div class="bs-callout bs-callout-info">
<ul>
<li>How might that assumption be violated?</li>
<li>Plot the residuals of the regression by district. Do they appear to be uncorrelated? What does that say about the validity of the OLS standard errors?</li>
<li>Do the standard errors match those reported in Table 1 of the article? What sort of standard errors does the article use?</li>
</ul>
</div>
</div>
<div id="regressions-with-log-slave-exports-per-capita" class="section level2">
<h2><span class="header-section-number">1.5</span> Regressions with log slave exports per capita</h2>
<p>Run the regression in Table 1, model 6, which uses “log(1 + exports / pop)” as a measure of slave exports.</p>
<pre class="r"><code>mod_1_6 &lt;- lm(trust_neighbors ~ ln_export_pop + 
              age + age2 + male + urban_dum + education +
              occupation + religion +
              living_conditions + district_ethnic_frac +
              frac_ethnicity_in_district + isocode,
              data = nunn)  </code></pre>
<div>
<ul>
<li>Interpret the effect of <code>ln_export_pop</code> on <code>trust_neighbors</code></li>
<li>Why is “log(1 + exports / pop)” used as the measure instead of “log(exports / pop)”?</li>
<li>Plot the fitted values of log(1 + exports / pop) and their confidence interval against “log(1 + exports / pop)” against the residuals of the controls only regression. Include the line, confidence intervals, and data points.</li>
<li>Plot the fitted values of exports / pop against the residuals of the controls only regression. Include the line, confidence intervals, and data points. How does this relationship differ from the one which used the level of slave exports with out taking the logarithm or adjusting for population?</li>
</ul>
</div>
</div>
<div id="sampling-distribution-of-ols-coefficients" class="section level2">
<h2><span class="header-section-number">1.6</span> Sampling distribution of OLS coefficients</h2>
<p>Let’s understand what the confidence intervals mean in terms of the sampling distribution. Since we don’t know the true parameter values for this, we will pretend that the OLS point estimates from the regression are the “true” population parameters.</p>
<p>The plan to generate a sampling distribution of <span class="math inline">\(\beta\)</span> is:</p>
<ol style="list-style-type: decimal">
<li>Draw a new sample <span class="math inline">\(\tilde{y}_i \sim N(\hat{y}_i, \hat{\sigma}^2)\)</span>.</li>
<li>Estimate OLS estimates <span class="math inline">\(\tilde{\vec{y}} = \tilde{vec{beta}} \mat{X} + \tilde{vec{varepsilon}} = \hat{y} + \tilde{\vec{\varepsilon}}\)</span>.</li>
<li>Repeat steps 1–2, <code>iter</code> times, store <span class="math inline">\(\beta^*\)</span> for each iteration, and return the estimates for all samples.</li>
</ol>
<p>Then, the distribution of the <span class="math inline">\(\beta^*\)</span> is a sampling distribution of the parameters.</p>
<div class="bs-callout bs-callout-info">
<p>Why is only <span class="math inline">\(\vec{y}\)</span> being samples? Why is <span class="math inline">\(\mat{X}\)</span> fixed in these simulations? See Wooldridge Ch 2 and 3 discussion of the assumptions of OLS.</p>
</div>
<p>Let’s take the results of the model on <code>ln_export_pop</code> and explore the sampling distribution of <span class="math inline">\(\beta\)</span> from that model.</p>
<p>First run the model,</p>
<pre class="r"><code>mod &lt;- lm(trust_neighbors ~ ln_export_pop + 
          age + age2 + male + urban_dum + education +
          occupation + religion +
          living_conditions + district_ethnic_frac +
          frac_ethnicity_in_district + isocode,
          data = nunn, na.action = na.exclude)</code></pre>
<p>The argument <code>na.action = na.exclude</code> ensures that when we calculate residuals, etc. they will be padded with missing values so they are the same length as the original <code>nunn</code> data. There are other ways to work around this, but this makes the code to run the simulations easier, especially the step that draws <code>y_hat</code>.</p>
<p>Extract the values of the parameter estimates, <span class="math inline">\(\hat{\beta}\)</span>, the model matrix, <span class="math inline">\(X\)</span>,the regression standard error, <span class="math inline">\(\hat{\sigma}\)</span>, and the number of observations, <span class="math inline">\(N\)</span>.</p>
<pre class="r"><code>y_hat &lt;- predict(mod, na.action = na.exclude)
sigma &lt;- sqrt(sum(residuals(mod) ^ 2, na.rm = TRUE) / mod$df.residual)
n &lt;- nrow(nunn)</code></pre>
<p>Later we’ll also need the original Choose a number of iterations to run. For this example use 1,024.</p>
<pre class="r"><code>iter &lt;- 1024</code></pre>
<p>Create a list to store the results</p>
<pre class="r"><code>results &lt;- vector(mode = &quot;list&quot;, length = iter)</code></pre>
<p>For iterations <code>1 ... iter</code> we cant to</p>
<ul>
<li>Draw the regression errors from i.i.d normal distributions, <span class="math inline">\(\tilde\epsilon_i \sim N(0, \hat\sigma^2)\)</span>.</li>
<li>Generate a new dependent variable, <span class="math inline">\(\tilde{\vec{y}} = \mat{X} \hat{\vec{\beta}} + \tilde{\vec{epsilon}}\)</span>.</li>
<li>Run an OLS regression to estimate the coefficients on the new data, <span class="math inline">\(\tilde{\vec{y}} = \mat{X} \tilde{\vec{\beta}} + \tilde{\vec{\epsilon}}\)</span></li>
<li>Save the <span class="math inline">\(\tilde{\beta}\)</span></li>
</ul>
<pre class="r"><code>p &lt;- progress_estimated(iter, min_time = 2)
for (i in seq_len(iter)) {
  # draw errors
  errors &lt;- rnorm(n, mean = 0, sd = sigma)
  # create new outcome variable from errors
  nunn[[&quot;trust_neighbors_new&quot;]] &lt;- y_hat + errors
  # Replace the dependent variable with the newly sampled y
  newmod &lt;- lm(trust_neighbors_new ~ ln_export_pop + 
            age + age2 + male + urban_dum + education +
            occupation + religion +
            living_conditions + district_ethnic_frac +
            frac_ethnicity_in_district + isocode,
            data = nunn)
  # Formula objects are manipulable in R. So a more general
  # way to do the above is to alter the formula from the model
  # by replacing trust_neighbors ~ ... with trust_neighbors_new ~ ...
  # formula &lt;- formula(mod$terms)
  # formula[[2]] &lt;- &quot;trust_neighbors_imputed&quot;
  # # check that this worked
  # # print(formula)
  # # Also this should be put outside the loop, since it doesn&#39;t
  # # change
  # newmod &lt;- lm(formula, data = nunn)
  # Save the coefficients as a data frame to the list
  results[[i]] &lt;- tidy(newmod) %&gt;% mutate(.iter = i)
  # Progress bar update
  p$tick()$print()
}
# clean up: remove the new variable
nunn[[&quot;trust_neighbors_new&quot;]] &lt;- NULL</code></pre>
<p>Finally, since <code>results</code> is a list of data frames, stack the data frames in the list to form a single data frame that is easier to analyze:</p>
<pre class="r"><code>results &lt;- bind_rows(results)</code></pre>
<p><strong>Note:</strong> this will take a few minutes.</p>
<div class="bs-callout bs-callout-info">
<p>Use the results of this simulation to answer the following questions:</p>
<ul>
<li>Plot the distribution of the coefficients of <code>ln_export_pop</code>.</li>
<li>What is the standard deviation of the sampling distribution of the coefficient of <code>ln_export_pop</code>? How does this compare to the standard error of this coefficient given by <code>lm()</code>?</li>
<li>Calculate the correlation matrix of the coefficients. For the first step will need to create a data frame using <code>spread</code> in which the rows are iterations, the columns are coefficients, and the values are the estimates.</li>
<li>Plot that correlation matrix using <code>geom_raster</code>. Are the coefficients of coefficients uncorrelated? In general, when would coefficients be more or less correlated?</li>
<li>Why is this simulation not (directly) appropriate for calculating a <span class="math inline">\(p\)</span>-value. What distribution would you have to simulate from to calculate a <span class="math inline">\(p\)</span>-value?</li>
</ul>
</div>
</div>
<div id="non-parametric-bootstrap" class="section level2">
<h2><span class="header-section-number">1.7</span> Non-parametric Bootstrap</h2>
<p>The previous question was an example of parametric bootstrap. It is a parametric bootstrap because you drew data from an assumed model (the OLS model that you estimated).</p>
<p>An alternative is a non-parametric bootstrap. In a non-parametric bootstrap, instead of drawing samples from model, we are going to redraw samples from the sample.</p>
<p>An analogy is that the sample is to the population as the bootstrap is to the sample. We are treating the sample distribution as an estimate of the population distribution and then drawing samples from that estimated population distribution.</p>
<p>To do the bootstrapping we will use the <code>bootstrap</code> function in the <strong>tidyr</strong> package. However, the <a href="https://cran.r-project.org/web/packages/boot/index.html">boot</a> package supports many more advanced methods of bootstrapping.</p>
<p>Let’s start by drawing a single bootstrap replication. It is a sample of the same size as the original data, drawn from the data <em>with replacement</em>.</p>
<pre class="r"><code>nunn_bootstrapped &lt;- bootstrap(nunn, 1)</code></pre>
<p>So, in order to calculate bootstrap standard errors, we will need to draw a sample of To get bootstrap standard errors, we draw <code>B</code> replications, run an regression, and save the estimates.</p>
<pre class="r"><code>beta_bs &lt;- 
  bootstrap(nunn, 1024) %&gt;%
    do(tidy(lm(trust_neighbors ~ ln_export_pop, data = .)))</code></pre>
<p>There are several ways to calculate standard errors from the bootstrap replications. The following are two simple methods.</p>
<ol style="list-style-type: decimal">
<li><p>Calculate the standard error from these simulations by taking the standard deviation of the estimates. Suppose <span class="math inline">\(\beta^{*b}_k\)</span> is the estimated coefficient from replication <span class="math inline">\(b \in 1:B\)</span>, and <span class="math inline">\(\bar\beta^{*}_k = (\sum \beta^{*b}_k) / B\)</span>. Then the bootstrap standard error is, <span class="math display">\[
   \se_k(\hat\beta_{k}) = \sqrt{\frac{1}{B - 1} \sum (\beta^{*b}_k - \bar\beta^{*b}_k)^2}
   \]</span> The confidence interval is thus, <span class="math display">\[
   \hat{\beta}_k \pm \se_{bs}(\hat\beta_k)
   \]</span> Note that you use the estimate <span class="math inline">\(\hat{\beta}_k\)</span> from the original model, not the mean of the bootstrap estimates. This method works well if the sampling distribution of <span class="math inline">\(\beta_k\)</span> is symmetric.</p></li>
<li><p>The second method is to use the quantiles of the bootstrap estimates. E.g. a 95% confidence interval uses the 2.5% and 97.5% quantiles of the bootstrap estimates. This method allows for asymmetric confidence intervals. However, it takes more replications to get accurate values of extreme quantiles than it does to calculate a standard deviation.</p></li>
</ol>
<div class="bs-callout bs-callout-info">
<ul>
<li>Estimate the bootstrapped confidence intervals using those two methods.</li>
<li>Compare the bootstrapped confidence intervals to the OLS confidence interval.</li>
</ul>
</div>
<p>There are even more advanced methods such as the studentized bootstrap, and the adjusted bootstrap percentile (BCa) methods included in <code>boot.ci</code>.</p>
<p>For bootstrapped standard errors to be valid, the samples from the data need to be taken in the same way as the sample was taken from the population. For example, in a time series it would be inappropriate to sample observations without accounting for their order.</p>
<div class="bs-callout bs-callout-info">
<ul>
<li>What is the population in this paper?</li>
<li>How was the sample drawn from this population?</li>
<li>In the previous examples, did we draw the sample in the same way as it was drawn from the population? What would be a better way of drawing the bootstrapped samples? Try to implement it; see the <code>group_by</code> argument of <code>bootstrap</code>.</li>
</ul>
</div>
</div>
<div id="f-test-example" class="section level2">
<h2><span class="header-section-number">1.8</span> F-test example</h2>
<p>An <span class="math inline">\(F\)</span>-test tests the null hypothesis that several coefficients in the regression are all 0 vs. the alternative that at least one of the coefficients is non-zero. Suppose you want to test that the <span class="math inline">\(q\)</span> coefficients <span class="math inline">\(\beta_j\)</span> through <span class="math inline">\(\beta_{j + q}\)</span> are all 0, <span class="math display">\[
\begin{aligned}[t]
H_0: &amp;\quad \beta_j = \dots = \beta_J = 0
H_a: &amp;\quad \text{at least one $\beta_k \neq 0$}
\end{aligned}
\]</span></p>
<p>To run an F-test in R, use the <code>anova()</code> function to compare two models. For example, to compare the regression of <code>trust_neighbors</code> on <code>exports</code> without controls to the regression with controls, use</p>
<pre class="r"><code>mod_1_0 &lt;- lm(trust_neighbors ~ ln_export_pop, data = nunn)
mod_1_1 &lt;- lm(trust_neighbors ~ ln_export_pop + age + age2 + male + urban_dum +
              education + occupation + religion + living_conditions +
              district_ethnic_frac + frac_ethnicity_in_district +
              isocode, data = nunn)
anova(mod_1_0, mod_1_1)</code></pre>
<pre><code>## Error in anova.lmlist(object, ...): models were not all fitted to the same size of dataset</code></pre>
<p>We can’t do it! At least not yet. The problem is that <code>lm()</code> drops all rows with at least one missing value. So <code>mod_1_0</code> and <code>mod_1_1</code> run a regression on datasets with different numbers of observations,</p>
<pre class="r"><code>mod_1_0$df.residual + length(mod_1_0$coefficients)</code></pre>
<pre><code>## [1] 18112</code></pre>
<pre class="r"><code>mod_1_1$df.residual + length(mod_1_1$coefficients)</code></pre>
<pre><code>## [1] 17644</code></pre>
<p>The residual degrees of freedom is <span class="math inline">\(N - K - 1\)</span>, and the length of the coefficient vector is <span class="math inline">\(K + 1\)</span>, so their sum is the number of observations in the regression, <span class="math inline">\((N - K - 1) + (K + 1) = N\)</span>.</p>
<p>To ensure that these models are run on the same set of data, create a subset of the <code>nunn</code> data that has all the variables in the larger model, and drop an observations with missing values in any of those variables, using <code>na.omit()</code>.</p>
<pre class="r"><code>nunn_nomiss &lt;- nunn %&gt;%
  select(trust_neighbors, ln_export_pop, age, age2, male, urban_dum,
         education, occupation, religion, living_conditions,
         district_ethnic_frac, frac_ethnicity_in_district,
         isocode) %&gt;%
  na.omit()</code></pre>
<p>Now, we can compare the models,</p>
<pre class="r"><code>mod_1_0 &lt;- lm(trust_neighbors ~ ln_export_pop, data = nunn_nomiss)
mod_1_1 &lt;- lm(trust_neighbors ~ ln_export_pop + age + age2 + male + urban_dum +
              education + occupation + religion + living_conditions +
              district_ethnic_frac + frac_ethnicity_in_district +
              isocode, data = nunn_nomiss)
anova(mod_1_0, mod_1_1)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: trust_neighbors ~ ln_export_pop
## Model 2: trust_neighbors ~ ln_export_pop + age + age2 + male + urban_dum + 
##     education + occupation + religion + living_conditions + district_ethnic_frac + 
##     frac_ethnicity_in_district + isocode
##   Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1  17642 17251                                  
## 2  17566 14777 76    2473.7 38.693 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The p-value for the F-test is approximately 0, so the test rejects the null hypothesis that all the control variables are equal to 0 at all commonly used levels of significance.</p>
<div class="bs-callout bs-callout-info">
<ul>
<li>Run and interpret an F-test for a reasonable subset of coefficients.</li>
<li>Why can’t you use F-tests to compare different models in Table 1?</li>
<li><p>Run an F-test comparing the model with only controls to the one in Model 1, Table 6. In other words, the null hypothesis is <span class="math inline">\(\beta_{\mathtt{ln\_exports\_pop}} = 0\)</span>.</p>
<pre class="r"><code>mod_controls &lt;- lm(trust_neighbors ~ age + age2 + male + urban_dum +
          education + occupation + religion + living_conditions +
          district_ethnic_frac + frac_ethnicity_in_district +
          isocode, data = nunn)    
mod_1_6 &lt;- lm(trust_neighbors ~ ln_export_pop + age + age2 + male + urban_dum +
          education + occupation + religion + living_conditions +
          district_ethnic_frac + frac_ethnicity_in_district +
          isocode, data = nunn)</code></pre>
How does the <span class="math inline">\(p\)</span>-value of the F-test compare to the p-value of the regression coefficient on <code>ln_export_pop</code>? Square the t-statistic for the coefficient of <code>ln_export_pop</code>; how does it compare to the F-statistic? What is the relationship between a t-test and an F-test for a single parameter in a regression?</li>
</ul>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This uses the <a href="https://rstudio.github.io/DT/">DT</a> package to produce pretty interactive tables in the HTML.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>See the discussion in (Scientific method: Statistical errors)[<a href="http://www.nature.com/news/scientific-method-statistical-errors-1.14700" class="uri">http://www.nature.com/news/scientific-method-statistical-errors-1.14700</a>], <em>Nature</em>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Assuming, for simplicity, that <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_a\)</span> are the only hypotheses so that <span class="math inline">\(p(H_0) + p(H_a) = 1\)</span>.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>This question is non-standard and idiosyncratic to the way I interpret research.<a href="#fnref4">↩</a></p></li>
</ol>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
