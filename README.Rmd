---
title: "Assignment 03"
output:
  html_document:
    keep_md: true
    self_contained: false
    css: styles.css
    number_sections: true
    includes:
      in_header: includes/in_header.html
      before_body: includes/before_body.html
---

Instructions

1. [Fork this repository](https://help.github.com/articles/using-pull-requests/) to your GitHub account.
2. Write your solutions in R Markdown in a file named `solutions.Rmd`.
3. When you are ready to submit your assignment, [initiate a pull request](https://help.github.com/articles/using-pull-requests/#initiating-the-pull-request). Title your
pull request "Submission".

To update your fork from the upstream repository:

1. On your fork, e.g. `https://github.com/jrnold/Assignment_03` click on "New Pull request"
2. Set your fork `jrnold/Assignment_03` as the base fork on the left, and `UW-POLS503/Assignment_03` as the head fork on the right. In both cases the branch will be master. This means, compare any canes in the head fork that are not in the base fork. You will see differences between the `US-POLS503` repository and your fork. Click on "Create Pull Request", and if there are no issues, "Click Merge" A quick way is to use this link, but change the `jrnold` to your own username: `https://github.com/jrnold/Assignment_03/compare/gh-pages...UW-POLS503:gh-pages`.

We'll use these packages,
```{r message=FALSE}
library("foreign")
library("dplyr")
library("broom")
library("ggplot2")
library("DT")
```
Since we are going to do some simulation, we should set a seed, so the results are exactly replicable.
```{r}
set.seed(1234)
```
Since some of these computations will take time, we can cache the results so that knitr will
only run code that has changed.
```{r}
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE)
```

# Nunn and Wantchekon AER 2011 example

Let's run some regressions from 

> Nunn, Nathan and Leonard Wantchekon. 2011. "The Slave Trade and the Origins of Mistrust in Africa."	American Economic Review,
> 101(7):3221-52. [doi:10.1257/aer.101.7.3221](https://dx.doi.org/10.1257/aer.101.7.3221)

The replication data for the is available from its [AER site](https://dx.doi.org/10.1257/aer.101.7.3221), but the main
dataset is included in this repository.
Since the main dataset is a Stata `.dta` file load it using the `read.dta` function
and convert it to a **dplyr** `tbl` so the `print` function produces nicer output.
```{r}
nunn <- read.dta("Nunn_Wantchekon_AER_2011.dta") %>% tbl_df()
```

There are many variables in this data.
When `read.dta` converts a Stata data file the descriptions of the variables end up
in an R [attribute](https://stat.ethz.ch/R-manual/R-devel/library/base/html/attributes.html) `"var.labels"`.
Print out the variable labels to get the descriptions of the files[^datatable]
```{r variable-labels,results = 'asis',cache=FALSE}
data_frame(variable = names(nunn), description = attr(nunn, "var.labels")) %>%
  datatable(class = 'cell-border stripe')
```

[^datatable]: This uses the [DT](https://rstudio.github.io/DT/) package to produce pretty interactive tables in the HTML.

In Table 1, NW run several models with Trust in Neighbors as an outcome variable,
different measures of slave exports as the treatment variable, and the same set of controls variables.
Some of the relevant variables in the data are:

- `trust_neighbors`: Trust of neighbors
- `exports`: Slave exports in 1000s
- Individual controls: `age`, `age2`, `male`, `urban_dum`, `education`, `occupation`, `religion`, `living_conditions`
- District controls: `district_ethnic_frac`, `frac_ethnicity_in_district`
- Country-fixed effects: `isocode`


## Bivariate regression

Run a regression of the Trust of Neighbors on Slave exports.
This is Table 1, Model 1, without any of the control variables.
```{r}
mod_1_0 <- lm(trust_neighbors ~ exports, data = nunn)
```

<div class="bs-callout bs-callout-info">
- Interpret the magnitude and statistical significance of the coefficient on `trust_neighbors`.
- Plot the fitted values and confidence interval of the fitted values of regression vs. `exports`.
- Plot the residuals of this regression against the fitted values of the regression. Do they appear to have
  constant variance? Are they approximately symmetric?
- What is the null hypothesis of the t-test reported by `summary()`? Explain the meaning of the p-value.
  Be precise. Is the p-value the probability that the null hypothesis is correct?
</div>

Example of using `augment` for fitted values:
```{r eval=FALSE}
augment(mod_1_0, data_frame(exports = seq(min(nunn$exports, na.rm = TRUE), max(nunn$exports, na.rm = TRUE), length.out = 100)))
```
Example of plotting the confidence intervals
```{r eval = FALSE}
ggplot() + geom_line(data = mod_1_0_fitted, mapping = aes(x = exports, y = .fitted)) + geom_ribbon(data = mod1_fitted, mapping = aes(x = exports, y = .fitted, ymin = .fitted - 2 * .se.fit, ymax = .fitted + 2 * .se.fit), alpha = 0.3) + geom_point(data = nunn, aes(x = exports, y = trust_neighbors))
```


## Probablities of Hypotheses

Frequentist statistics assigns no probabilities to hypotheses (parameter values).
They are either true or false, but they are unknown. Only samples are random variables, and have an associated probability.
But as scientists, we are generally interested in the probability that a hypothesis is correct.[^nature]
The probability that the research hypothesis ($H_0$) is correct can be calculated with Bayes law,
$$
p(H_0 | \text{data}) =
\frac{p(\text{data} | H_0) p(H_0)}{p(\text{data} | H_a) p(H_a) + p(\text{data} | H_0) p(H_0)} = \frac{p(\text{data} | H_0) p(H_0)}{p(\text{data})}
$$
Working somewhat informally, the p-value gives $p(\text{data} | H_0)$. An important missing piece of information is the baseline or prior probability that the null hypothesis is true, $p(H_0)$, which is the complement of the probability that the research hypothesis is true, $p(H_0) = 1 - p(H_a)$,[^h0] [^jeff]

<div class="bs-callout bs-callout-info">
- If more than the p-value is required to make sense of the research findings, what does the article do to increase your belief about the research hypothesis, $p(H_a)$?
- Suppose you believed that NW were p-value hacking (which I don't think they are!). What part of Bayes law is that 
  affecting? If you think that someone is p-value hacking, then you are saying that they will always produce significant p-values regardless of whether the null or alternative hypotheses are true.
</div>

[^nature]: See the discussion in (Scientific method: Statistical errors)[http://www.nature.com/news/scientific-method-statistical-errors-1.14700], *Nature*.

[^jeff]: This question is non-standard and idiosyncratic to the way I interpret research.

[^h0]: Assuming, for simplicity, that $H_0$ and $H_a$ are the only hypotheses so that $p(H_0) + p(H_a) = 1$.


## Multiple regression

In the models in Table 1, NW includes control variables to account for individual, district, and country-level 
variables that may explain differences. 

Run the model in Table 1, Model 1:
```{r}
mod_1_1 <- lm(trust_neighbors ~ exports + 
              age + age2 +
              male + urban_dum + factor(education) + 
              factor(occupation) + factor(religion) +
              factor(living_conditions) + district_ethnic_frac +
              frac_ethnicity_in_district + isocode,
              data = nunn)
mod_1_1
```

<div class="bs-callout bs-callout-info">
- Interpret the coefficient on `exports`
- How much does the coefficient change with the addition of control variables? What does that suggest?
- Do the R^2 and number of observations match those reported in Table 1?
- Calculate the fitted values of the regression by multiplying the $\beta$ vector and the $\mat{X}$ matrix.
  Confirm that you get the same results as using `predict()`.
- How would you create a plot that shows the predicted values of `trust_neighbors` as the value of `exports` changes?
  What is different about the multiple regression case than the bivariate case?
</div>


### Understanding Multiple Regression

<div class="bs-callout bs-callout-info">
- Run the following regressions
    1. Regress regression of `trust_neighbors` on the controls.
        ```r
        lm(trust_neighbors ~ age + age2 +
           male + urban_dum + factor(education) + 
           factor(occupation) + factor(religion) +
           factor(living_conditions) + district_ethnic_frac +
           frac_ethnicity_in_district + isocode, data = nunn)
        ```
        Save the residuals.
    2. Run the regression of `exports` on the controls. Save the residuals
        ```r
        lm(exports ~ age + age2 +
           male + urban_dum + factor(education) + 
           factor(occupation) + factor(religion) +
           factor(living_conditions) + district_ethnic_frac +
           frac_ethnicity_in_district + isocode, data = nunn)
        ```
        Save the residuals.
    3. Regress the residuals from 1. on the residuals on 2.
- How does the coefficient from regression 3 compare the the coefficient on `exports` from the regression in Table 1, Model 1?
    What does that say about what multiple regression is doing?
- Are the steps 
</div>


## Validity of the standard errors

One of the assumptions necessary for OLS standard errors to be correct is homoskedasticity homoskedasticity (constant variance), and that the errors are uncorrelated.

<div class="bs-callout bs-callout-info">
- How might that assumption be violated?
- Plot the residuals of the regression by district. Do they appear to be uncorrelated? What does that say about the validity of the OLS standard errors?
- Do the standard errors match those reported in Table 1 of the article? What sort of standard errors does the article use?
</div>


## Regressions with log slave exports per capita

Run the regression in Table 1, model 6, which uses "log(1 + exports / pop)" as a measure of slave exports.

```{r}
mod_1_6 <- lm(trust_neighbors ~ ln_export_pop + 
              age + age2 + male + urban_dum + factor(education) +
              factor(occupation) + factor(religion) +
              factor(living_conditions) + district_ethnic_frac +
              frac_ethnicity_in_district + isocode,
              data = nunn)  
```


<div>
- Interpret the effect of `ln_export_pop` on `trust_neighbors`
- Why is "log(1 + exports / pop)" used as the measure instead of "log(exports / pop)"?
- Plot the fitted values of log(1 + exports / pop) and their confidence interval against "log(1 + exports / pop)" against the residuals of the controls only regression. Include the line, confidence intervals, and data points.
- Plot the fitted values of exports / pop against the residuals of the controls only regression. Include the line, confidence intervals, and data points. How does this relationship differ from the one which used the level of slave exports with out taking the logarithm or adjusting for population?
</div>


## Sampling distribution of OLS coefficients

Let's understand what the confidence intervals mean in terms of the sampling distribution.
Since we don't know the true parameter values for this, we will pretend that the OLS point estimates from the regression are the "true" population parameters.

The plan to generate a sampling distribution of $\beta$ is:

1. Draw a new sample $\tilde{y}_i \sim N(\hat{y}_i, \hat{\sigma}^2)$.
2. Estimate OLS estimates $\tilde{\vec{y}} = \tilde{vec{beta}} \mat{X} + \tilde{vec{varepsilon}} = \hat{y} + \tilde{\vec{\varepsilon}}$.
3. Repeat steps 1--2, `iter` times, store $\beta^*$ for each iteration, and
   return the estimates for all samples.
   
Then, then distribution of the $\beta^*$ is a sampling distribution of the parameters.

<div class="bs-callout bs-callout-info">
Why is only $\vec{y}$ being samples? Why is $\mat{X}$ fixed in these simulations? See Wooldridge Ch 2 and 3 discussion of the assumptions of OLS.
</div>

Let's take the results of the model on `ln_exports_pop` and explore the sampling distribution of $\beta$ from that model.

First run the model,
```{r}
mod <- lm(trust_neighbors ~ ln_export_pop + 
          age + age2 + male + urban_dum + factor(education) +
          factor(occupation) + factor(religion) +
          factor(living_conditions) + district_ethnic_frac +
          frac_ethnicity_in_district + isocode,
          data = nunn) 
```
Extract the values of the parameter estimates, $\hat{\beta}$, the model matrix, $X$,the regression standard error, $\hat{\sigma}$, and the number of observations, $N$.
```{r}
y_hat <- predict(mod)
sigma <- sqrt(sum(mod$residuals ^ 2) / mod$df.residual)
n <- nrow(X)
```
Later we'll also need the original 
Choose a number of iterations to run. 
For this example use 1,024.
```{r}
iter <- 1024
```
Create a list to store the results
```{r}
results <- vector(mode = "list", length = iter)
```

- Draw the regression errors from i.i.d normal distributions, $\tilde\epsilon_i \sim N(0, \hat\sigma^2)$.
- Generate a new dependent variable, $\tilde{\vec{y}} = \mat{X} \hat{\vec{\beta}} + \tilde{\vec{epsilon}}$.
- Run the OLS regression to estimate the coefficients on the new data, $\tilde{\vec{y}} = \mat{X} \tilde{\vec{\beta}} + \tilde{\vec{\epsilon}}$

```{r}
# Create a copy of the data to use in the regression
for (i in seq_len(iter)) {
  # draw errors
  errors <- rnorm(n, mean = 0, sd = sigma)
  # create new outcome variable from errors
  y <- y_hat + errors
  # Replace the dependent variable with the newly sampled y
  # re-run the original regression with the new data
  # the update function re-runs the regression with changes
  # to the regression formula. In this case, we will use
  # y as the dependent variable instead of ln_exports_pop
  # but use all the same predictors (the "." on the right hand side)
  newmod <- update(mod, y ~ .)
  # Save the coefficients as a data frame to the list
  results[[i]] <- tidy(newmod) %>% mutate(.iter = i)
}
```
Finally, since `results` is a list of data frames, stack the data frames in the list to form a single data frame that is easier to analyze:
```{r}
results <- bind_rows(results)
```

<div class="bs-callout bs-callout-info">
- Plot the distribution of the coefficients of `ln_export_pop`. Use a density function 
- What is the standard deviation of the sampling distribution of the coefficient of `ln_export_pop`? How does this compare to the standard error of this coefficient given by `lm()`?
- Calculate the correlation matrix of the coefficients. For the first step will need to create a data frame using `spread` in which the rows are iterations, the columns are coefficients, and the values are the esimates.
- Plot that correlation matrix using `geom_raster`. Are the coefficients of coefficients uncorrelated? When would coefficients be uncorrelated?
</div>


## Bootstrapping

The previous question was an example of parametric bootstrap.
It is a parametric bootstrap because you drew data from an assumed model (the OLS model that you estimated).

There is also a non-parametric bootstrap.
In a non-parametric bootstrap, instead of drawing new samples from the fitted model, we are going to draw new samples from the original sample itself.

In a bootstrap, we are treating the sample distribution as an estimate of the unknown population distribution and then drawing new samples from that estimated population distribution.
An analogy for bootstrap is that the original sample is to the population as the bootstrap replications are to the sample.

To do the bootstrapping we will use the `bootstrap` function in the **tidyr** package.
However, the [boot](https://cran.r-project.org/web/packages/boot/index.html) package supports many more advanced methods of bootstrapping.

Let's start by drawing a single bootstrap replication.
It is a sample of the same size as the original data, drawn from the data *with replacement*.
```{r message=FALSE, results='hide'}
nunn_bootstrapped <- bootstrap(nunn, 1)
```

So, in order to calculate bootstrap standard errors, we will need to draw a sample of 
To get bootstrap standard errors, we draw `B` replications, run an  regression, and save the estimates. 
```{r message=FALSE, results='hide'}
beta_bs <- 
  bootstrap(nunn, 1024) %>%
    do(tidy(lm(trust_neighbors ~ exports, data = nunn)))
```

There are several ways to calculate standard errors from the bootstrap replications.
The following are two simple methods.

1. Calculate the standard error from these simulations by taking the standard deviation of the estimates.
   Suppose $\beta^{*b}_k$ is the estimated coefficient from replication $b \in 1:B$, and $\bar\beta^{*}_k = (\sum \beta^{*b}_k) / B$.
   Then the bootstrap standard error is,
   $$
   \se_k(\hat\beta_{k}) = \sqrt{\frac{1}{B - 1} \sum (\beta^{*b}_k - \bar\beta^{*b}_k)^2}
   $$
   The confidence interval is thus,
   $$
   \hat{\beta}_k \pm \se_{bs}(\hat\beta_k)
   $$
   Note that you use the estimate $\hat{\beta}_k$ from the original model, not the mean of the bootstrap estimates.
   This method works well if the sampling distribution of $\beta_k$ is symmetric.

2. The second method is to use the quantiles of the bootstrap estimates.
   E.g. a 95% confidence interval uses the 2.5% and 97.5% quantiles of the bootstrap estimates.
   This method allows for asymmetric confidence intervals. However, it takes more replications to get accurate 
   values of extreme quantiles than it does to calculate a standard deviation.

<div class="bs-callout bs-callout-info">
- Estimate the bootstrapped confidence intervals using those two methods.
- Compare the bootstrapped confidence intervals to the OLS confidence interval.
</div>

There are even more advanced methods such as the studentized bootstrap, and the adjusted bootstrap percentile (BCa) methods
included in `boot.ci`.

For bootstrapped standard errors to be valid, the samples from the data need to be taken in the same way as the sample
was taken from the population. 
For example, in a time series it would be inappropriate to sample observations without accounting for their order.

<div class="bs-callout bs-callout-info">
- What is the population in this paper?
- How was the sample drawn from this population?
- In the previous examples, did we draw the sample in the same way as it was drawn from the population? What would
  be a better way of drawing the bootstrapped samples?
  Try to implement it; see the `group_by` argument of `bootstrap`.
</div>

## F-test example

**TODO**

An $F$-test tests the null hypothesis that several coefficients in the regression are all 0 vs. the alternative that 
at least one of the coefficients is non-zero.

$$
\begin{aligned}[t]
H_0: &\quad \beta_j = \dots = \beta_J = 0
H_a: &\quad \text{at least one $\beta_k \neq 0$}
\end{aligned}
$$

To run an F-test in R, use the `anova()` function to compare two models.
For example, to compare the regression of `trust_neighbors` on `exports` without controls to the regression with controls, use
```{r}
anova(mod_1_0, mod_1_1)
```

<div 
- Run and interpret an F-test with 
- Can you use the F-test to compare model 
</div>



### F-test simulations

To better understand the F-test, let's run some simulations. 

First, we need to simulate data from the null hypothesis.
In the null hypothesis, $H_0 = 0$ for all control variables.

1. Simulate $\tilde{\varepsilon}$ from $N(0, \hat{\sigma}^2)$
2. Run the regression
2. Calculate the F-statistic



# Multiple comparisons and the F-test

This question compares an F-test to running multiple t-tests

```{r results='hide'}
noise <- data.frame(matrix(rnorm(2100), nrow = 100, ncol = 21))
summary(lm(noise))
```

<div class="bs-callout bs-callout-info">
- How many variables have t-tests that are significant? Is this to be expected?
- Is the F-test significant? 
- What is the null and alternative hypotheses of the F-test? What are the implicit
  null and alternative hypotheses of running all those t-tests? What explains the
  difference in results?
</div>
